{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/hdp/2.6.5.0-292/spark2')\n",
    "\n",
    "# Create a Spark Context which will be used for distributed data processing\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Twitter Topic Sentiment\")\n",
    "\n",
    "import string\n",
    "\n",
    "import re as re\n",
    "\n",
    "import nltk\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel, StopWordsRemover\n",
    "\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyspark.mllib.linalg import Vector as oldVector, Vectors as oldVectors\n",
    "\n",
    "from pyspark.ml.linalg import Vector as newVector, Vectors as newVectors\n",
    "\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SQL Context which will be used for sql like distriburted data processing\n",
    "\n",
    "# As I get more familiar with what technology to use where I will be switching between using pyspard RDDs,\n",
    "\n",
    "# pyspark dataframes, and pandas dataframes\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop is the filesystem being used. This is a three node virtual cluster\n",
    "\n",
    "# Read in data from Hadoop\n",
    "\n",
    "ITData = sc.textFile(\"hdfs:////user/vagrant/practicum/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output sample of data\n",
    "\n",
    "ITData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of records loaded to pyspark RDD\n",
    "\n",
    "ITData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, data is partitioned based on the data size\n",
    "\n",
    "# Check the number of partitions created\n",
    "\n",
    "ITData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter data was collected and batched in files with each file having a file header\n",
    "\n",
    "# Extract the first file header from the dataset and display\n",
    "\n",
    "# This will be used later to remove all headers from the dataset\n",
    "\n",
    "header = ITData.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all of the headers from the data set\n",
    "\n",
    "# Count the number of records remaining in the data set\n",
    "\n",
    "# If 10 files were read from Hadoop, this count should be 10 less\n",
    "\n",
    "ITData_NoHeader = ITData.filter(lambda row : row != header)\n",
    "ITData_NoHeader.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have an RDD with not header information\n",
    "\n",
    "# In preparation for creating a dataframe from the RDD, create a schema based on the original header\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('timetext', StringType(), nullable=True),\n",
    "    StructField('tweet_id', StringType(), nullable=True),\n",
    "    StructField('tweet_source', StringType(), nullable=True),\n",
    "    StructField('tweet_truncated', StringType(), nullable=True),\n",
    "    StructField('tweet_text', StringType(), nullable=True),\n",
    "    StructField('tweet_user_screen_name', StringType(), nullable=True),\n",
    "    StructField('tweet_user_id', StringType(), nullable=True),\n",
    "    StructField('tweet_user_location', StringType(), nullable=True),\n",
    "    StructField('tweet_user_description', StringType(), nullable=True),\n",
    "    StructField('tweet_user_followers_count', StringType(), nullable=True),\n",
    "    StructField('tweet_user_statuses_count', StringType(), nullable=True),\n",
    "    StructField('tweet_user_time_zone', StringType(), nullable=True),\n",
    "    StructField('tweet_user_geo_enabled', StringType(), nullable=True),\n",
    "    StructField('tweet_user_lang', StringType(), nullable=True),\n",
    "    StructField('tweet_coordinates_coordinates', StringType(), nullable=True),\n",
    "    StructField('tweet_place_country', StringType(), nullable=True),\n",
    "    StructField('tweet_place_country_code', StringType(), nullable=True),\n",
    "    StructField('tweet_place_full_name', StringType(), nullable=True),\n",
    "    StructField('tweet_place_name', StringType(), nullable=True),\n",
    "    StructField('tweet_place_type', StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Create a dataframe from the RDD with schema\n",
    "\n",
    "ITData_df = sqlContext.createDataFrame(ITData_NoHeader.map(lambda s: s.split(\",\")), schema)\n",
    "\n",
    "ITData_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert dataframe to rdd\n",
    "\n",
    "# Use map lambda to select the tweet_text column and filter out all empty records\n",
    "\n",
    "tweet = ITData_df.rdd.map(lambda x: x['tweet_text']).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stop words. Note we may need to add to the stop words list based on topic model results\n",
    "\n",
    "StopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further clean tweets, split them out into individual words, and number them by adding an index\n",
    "\n",
    "tokens = tweet.map(lambda document: document.strip().lower()) \\\n",
    "              .map(lambda document: re.split(\" \", document)) \\\n",
    "              .map(lambda word: [x for x in word if x.isalpha()]) \\\n",
    "              .map(lambda word: [x for x in word if len(x) > 3]) \\\n",
    "              .map(lambda word: [x for x in word if x not in StopWords]) \\\n",
    "              .zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens is an RDD, display the first 5 records\n",
    "\n",
    "tokens.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe from the above RDD, adding column names\n",
    "\n",
    "tweet_df = sqlContext.createDataFrame(tokens, [\"tweet_words\", 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 records of the dataframe\n",
    "\n",
    "tweet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Topic Modeling\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "cv = CountVectorizer(inputCol=\"tweet_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=10.0)\n",
    "cvmodel = cv.fit(tweet_df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "result_cv = cvmodel.transform(tweet_df)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cv.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = result_cv.rdd.map(lambda (x, y, z): (x, y, oldVectors.fromML(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = rs.toDF(['tweet_words', 'index', 'raw_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA Topic Modeler\n",
    "\n",
    "# Note the time before and after is printed in order to find out how much time it takes to process x number of records\n",
    "\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "num_topics = 10\n",
    "max_iterations = 20\n",
    "lda_model = LDA.train(rs_df['index', 'raw_features'].rdd.map(list), k=num_topics, maxIterations=max_iterations)\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabArray = cvmodel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the top number of topics to write to spark\n",
    "\n",
    "wordNumbers = 20\n",
    "topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_render(topic):\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%m%d%Y %H:%M:%S'))\n",
    "topics_final = topicIndices.map(lambda topic:\n",
    "                               topic_render(topic)).collect()\n",
    "print(time.strftime('%m%d%Y %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print(\"Topic\" + str(topic) + \":\")\n",
    "    for term in topics_final[topic]:\n",
    "        print(term)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above above relates topics to the terms I searched in Twitter\n",
    "\n",
    "# For sentiment analysis, I would like to rate the actual search terms.\n",
    "\n",
    "# For this I will build a python array with those search terms\n",
    "\n",
    "search_terms = [\"machine_learning\", \"computer_programmer\", \"database_engineer\", \"network_engineer\", \\\n",
    "                \"data_scientist\", \"systems_engineer\", \"data_analyst\", \"data_architect\", \"etl_architect\", \\\n",
    "                \"web_programmer\", \"automation_engineer\", \"data_processing\", \"application_engineer\", \\\n",
    "                \"software_engineer\", \"software_developer\", \"information_architect\", \"security_analyst\", \\\n",
    "                \"business_intelligence\", \"enterprise_architect\", \"solution_architect\", \"data_warehouse\", \\\n",
    "                \"information_technology\", \"data\", \"java\", \"iot\", \"computer\", \"systems\", \"technology\", \\\n",
    "                \"etl\", \"devops\", \"cloud\", \"developer\", \"programmer\", \"ai\"]\n",
    "\n",
    "search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function to search for topics within a tweet\n",
    "\n",
    "# Function will return the topic and the related tweet or NA is no topic found and the related tweet\n",
    "\n",
    "def SearchTopics(topics, tweet_text):\n",
    "    for term in topics:\n",
    "        result = tweet_text.find(term)\n",
    "        if result > -1:\n",
    "            return term, tweet_text\n",
    "    return 'NA', tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While removing stopwords helps obtain valid topics it will not help with sentiment analysis\n",
    "\n",
    "# With topics in hand, topics_final, we will use tweets where stop words have not been removed\n",
    "\n",
    "tweet.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search each tweet for topics returning only tweets that match\n",
    "\n",
    "# SearchTopics will return both the topic and the related tweet\n",
    "\n",
    "# Sentiment will be done on these tweets\n",
    "\n",
    "topic_tweet = tweet.map(lambda x: SearchTopics(search_terms, x)).filter(lambda x: x[0] != 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 topic tweet combinations\n",
    "\n",
    "topic_tweet.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup sentiment analysis\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function to print the sentiment scores\n",
    "\n",
    "# This function will have topic and related tweet as in put\n",
    "\n",
    "# This function will perform sentiment analysis and output topic, tweet, and sentiment\n",
    "\n",
    "# Also note this function will only return the compound portion of the sentiment\n",
    "\n",
    "# Revert sigpipe to default behavior\n",
    "\n",
    "def print_sentiment_scores(topic, sentence):\n",
    "    snt = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "    print(str(snt))\n",
    "    return(topic, sentence, str(snt.get('compound')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve sentiment for each topic, tweet\n",
    "\n",
    "topic_tweet_sentiment = topic_tweet.map(lambda x: print_sentiment_scores(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sentiment\n",
    "\n",
    "topic_tweet_sentiment.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the topic and sentiment only\n",
    "\n",
    "topic_tweet_sentiment_pair = topic_tweet_sentiment.map(lambda x: (x[0], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topic, sentiment combination\n",
    "\n",
    "topic_tweet_sentiment_pair.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe naming columns\n",
    "\n",
    "topic_tweet_sentiment_pair_df = topic_tweet_sentiment_pair.toDF(['topic', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display dataframe\n",
    "\n",
    "topic_tweet_sentiment_pair_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count sentiment records\n",
    "\n",
    "topic_tweet_sentiment_pair_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create panda dataframe based on topic, sentiment dataframe\n",
    "\n",
    "# This dataframe will enable us to plot highs, lows, and means\n",
    "\n",
    "pdf1 = topic_tweet_sentiment_pair_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new dataframe types\n",
    "\n",
    "pdf1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment is currently of type object, needs to be float\n",
    "\n",
    "# Convert sentiment datatype to float\n",
    "\n",
    "pdf1['sentiment'] = pdf1.sentiment.astype(float)\n",
    "\n",
    "# Check datatypes\n",
    "\n",
    "pdf1.dtypes\n",
    "\n",
    "# list new panda dataframe\n",
    "\n",
    "pdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data\n",
    "\n",
    "pdf1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1.groupby('topic').groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1_group_counts = pdf1.groupby(['topic'])[['sentiment']].count()\n",
    "pdf1_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1_mean = pdf1.groupby('topic', as_index=False).agg({\"sentiment\": \"mean\"})\n",
    "pdf1_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barchart\n",
    "\n",
    "pdf1_plot = pdf1_group_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot sentiments by topic\n",
    "\n",
    "pdf1.boxplot(by='topic', column=['sentiment'], grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_terms1 = ['ai', 'data', 'tecnology', 'cloud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = pdf1[pdf1.topic.isin(sentiment_terms1)]\n",
    "pdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.groupby('topic').groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2_group_counts = pdf2.groupby(['topic'])[['sentiment']].count()\n",
    "pdf2_group_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2_mean = pdf2.groupby('topic', as_index=False).agg({\"sentiment\": \"mean\"})\n",
    "pdf2_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barchart\n",
    "\n",
    "pdf2_plot = pdf2_group_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot sentiments by topic\n",
    "\n",
    "pdf2.boxplot(by='topic', column=['sentiment'], grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
